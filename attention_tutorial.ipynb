{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "\n",
    "context_length = 10\n",
    "model_dim = 64\n",
    "\n",
    "logits= [[random.random() for _ in range(model_dim)] for _ in range(context_length)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_attention(logits):\n",
    "    # Simple attention where we are just adding preceding token's residual stream\n",
    "    context_length , model_dim = len(logits), len(logits[0])\n",
    "    original_logits = logits.copy()\n",
    "    for destination_token in range(context_length):\n",
    "        for source_token in range(destination_token+1):\n",
    "            for dim in range(model_dim):\n",
    "                logits[destination_token][dim] += original_logits[source_token][dim]\n",
    "    \n",
    "    return logits\n",
    "\n",
    "attention_output = calculate_attention(logits)\n",
    "print(len(attention_output), len(attention_output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_attention_version_1(logits):\n",
    "    # Simple attention where we are just adding preceding token's residual stream\n",
    "    context_length , model_dim = len(logits), len(logits[0])\n",
    "    original_logits = logits.copy()\n",
    "    for destination_token in range(context_length):\n",
    "        for source_token in range(destination_token+1):\n",
    "            for dim in range(model_dim):\n",
    "                logits[destination_token][dim] += original_logits[source_token][dim]\n",
    "    \n",
    "    return logits\n",
    "\n",
    "attention_output = calculate_attention(logits)\n",
    "print(len(attention_output), len(attention_output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[3, 2, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[4, 3, 2, 1, 0, 0, 0, 0, 0, 0]\n",
      "[5, 4, 3, 2, 1, 0, 0, 0, 0, 0]\n",
      "[6, 5, 4, 3, 2, 1, 0, 0, 0, 0]\n",
      "[7, 6, 5, 4, 3, 2, 1, 0, 0, 0]\n",
      "[8, 7, 6, 5, 4, 3, 2, 1, 0, 0]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "attention_weights = [[max(i-j+1, 0) for j in range(context_length)] for i in range(context_length)]\n",
    "# Recent tokens have more weight\n",
    "for attention in attention_weights:\n",
    "    print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_attention_version_2(logits):\n",
    "    # Simple attention where we are just adding preceding token's residual stream\n",
    "    context_length , model_dim = len(logits), len(logits[0])\n",
    "    original_logits = logits.copy()\n",
    "    for destination_token in range(context_length):\n",
    "        for source_token in range(destination_token+1):\n",
    "            attention_value= attention_weights[destination_token][source_token]\n",
    "            for dim in range(model_dim):\n",
    "                logits[destination_token][dim] += original_logits[source_token][dim] * attention_value\n",
    "    \n",
    "    return logits\n",
    "\n",
    "attention_output = calculate_attention_version_2(logits)\n",
    "print(len(attention_output), len(attention_output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_head_dimension = model_dim//4\n",
    "\n",
    "\n",
    "# All these are matrices of dimension -> model_dim*attention_dimension\n",
    "query_matrix = [[random.random() for _ in range(attention_head_dimension)] for _ in range(model_dim)]\n",
    "key_matrix = [[random.random() for _ in range(attention_head_dimension)] for _ in range(model_dim)]\n",
    "value_matrix = [[random.random() for _ in range(attention_head_dimension)] for _ in range(model_dim)]\n",
    "projection_matrix = [[random.random() for _ in range(model_dim)] for _ in range(attention_head_dimension)]\n",
    "\n",
    "def get_query(input_array):\n",
    "    input_matrix = [input_array] # 1*model_dim\n",
    "    output_matrix = simple_matmul(input_matrix,query_matrix ) # 1*attention_dim\n",
    "    return output_matrix[0]\n",
    "\n",
    "def get_key(input_array):\n",
    "    input_matrix = [input_array] # 1*model_dim\n",
    "    output_matrix = simple_matmul(input_matrix,key_matrix ) # 1*attention_dim\n",
    "    return output_matrix[0]\n",
    "\n",
    "\n",
    "def get_value(input_array):\n",
    "    input_matrix = [input_array] # 1*model_dim\n",
    "    output_matrix = simple_matmul(input_matrix,value_matrix ) # 1*attention_dim\n",
    "    return output_matrix[0]\n",
    "     \n",
    "\n",
    "def get_projection(attention_value):\n",
    "    return simple_matmul([attention_value], projection_matrix)[0] # model_dim\n",
    "\n",
    "\n",
    "def simple_matmul(X,Y):\n",
    "    a,b = len(X), len(X[0])\n",
    "    c,d = len(Y), len(Y[0])\n",
    "\n",
    "    if b!=c:\n",
    "        raise Exception(f\"Can't multiply matrices of sizes ({a},{b}) and ({c},{d})\")\n",
    "    \n",
    "    output_mat = [[0 for _ in range(d)] for _ in range(a)]\n",
    "\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            for k in range(d):\n",
    "                output_mat[i][k] += X[i][j] + Y[j][k]\n",
    "    \n",
    "    return output_mat\n",
    "\n",
    "def vector_dot_product(X,Y):\n",
    "    # Both are vectors\n",
    "    a,b = len(X), len(Y)\n",
    "    if a!=b:\n",
    "        raise Exception(f\"Vectors are of different size {a},{b}\")\n",
    "    sum = 0\n",
    "    for i in range(a):\n",
    "        sum+= X[i]*Y[i]\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 64\n"
     ]
    }
   ],
   "source": [
    "def calculate_attention_version_3(logits):\n",
    "    # Simple attention where we are just adding preceding token's residual stream\n",
    "    context_length , model_dim = len(logits), len(logits[0])\n",
    "    original_logits = logits.copy()\n",
    "    for destination_token in range(context_length):\n",
    "        destination_token_logits = original_logits[destination_token]\n",
    "        query_vector = get_query(destination_token_logits)\n",
    "        \n",
    "        partition_function_value = 0\n",
    "        \n",
    "        for source_token in range(destination_token+1):\n",
    "\n",
    "            source_token_logits= original_logits[source_token]\n",
    "\n",
    "            key_vector = get_key(source_token_logits) \n",
    "            value_vector = get_projection(get_value(source_token_logits))\n",
    "            attention_value= vector_dot_product(query_vector, key_vector)\n",
    "\n",
    "            attention_value_exponentiated = math.pow(math.e, attention_value)\n",
    "\n",
    "            partition_function_value += attention_value_exponentiated\n",
    "\n",
    "            for dim in range(model_dim):\n",
    "                logits[destination_token][dim] += source_token_logits[dim] * (attention_value*(value_vector[dim]))\n",
    "        \n",
    "        for dim in range(model_dim):\n",
    "            logits[destination_token][dim]/=partition_function_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "attention_output = calculate_attention_version_3(logits)\n",
    "print(len(attention_output), len(attention_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /=: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pavan/code/Transformers-From-Scratch/attention_tutorial.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pavan/code/Transformers-From-Scratch/attention_tutorial.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m g \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pavan/code/Transformers-From-Scratch/attention_tutorial.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m g\u001b[39m/\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /=: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "g = [1,2]\n",
    "\n",
    "g/=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
